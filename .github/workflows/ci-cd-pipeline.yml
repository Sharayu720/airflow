name: Deploy Apache Airflow to Test Environment

on:
  push:
    branches:
      - test

jobs:
  deploy:
    runs-on: sharayu_test  # Use self-hosted runner

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Install Python 3.9 and dependencies on EC2 (via SSH)
        run: |
          # Save private SSH key to file and set appropriate permissions
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > /tmp/id_rsa
          chmod 600 /tmp/id_rsa
          
          # SSH into EC2 instance and install dependencies
          ssh -v -o StrictHostKeyChecking=no -i /tmp/id_rsa ec2-user@${{ secrets.EC2_PRIVATE_IP }} << 'EOF'
            set -e  # Exit immediately if a command fails

            echo "Updating system and installing Python 3.9 dependencies..."

            # Check if Python 3.9 is already installed
            if ! command -v python3.9 &>/dev/null; then
              echo "Python 3.9 is not installed. Installing..."
              sudo dnf update -y
              sudo dnf groupinstall "Development Tools" -y
              sudo dnf install python39 python39-venv python39-devel -y
            else
              echo "Python 3.9 is already installed."
            fi

            # Check installed Python version
            python3.9 --version

            # Set up virtual environment
            python3.9 -m venv /home/ec2-user/airflow/airflow_venv
            source /home/ec2-user/airflow/airflow_venv/bin/activate

            # Upgrade pip and install requirements
            pip install --upgrade pip
            pip install -r /home/ec2-user/airflow/requirements.txt
          EOF

      - name: Set up Python 3.9 virtual environment and install dependencies locally
        run: |
          # Set up virtual environment locally (for testing before SSH)
          python3.9 -m venv airflow_venv
          source airflow_venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: SSH to EC2 instance and configure Airflow
        run: |
          # SSH into EC2 instance and configure Airflow (init DB, create user, start services)
          ssh -v -o StrictHostKeyChecking=no -i /tmp/id_rsa ec2-user@${{ secrets.EC2_PRIVATE_IP }} << 'EOF'
            set -e  # Exit immediately if a command fails

            echo "Setting up Airflow directories and permissions..."
            sudo mkdir -p /home/ec2-user/airflow/logs/scheduler
            sudo mkdir -p /home/ec2-user/airflow/logs/webserver
            sudo chown -R ec2-user:ec2-user /home/ec2-user/airflow
            sudo chmod -R 777 /home/ec2-user/airflow/logs

            echo "Initializing Airflow DB..."
            source /home/ec2-user/airflow/airflow_venv/bin/activate
            export AIRFLOW_HOME=/home/ec2-user/airflow
            airflow db init

            echo "Creating Airflow Admin user..."
            airflow users create --username admin --firstname John --lastname Doe --email john.doe@example.com --role Admin --password "${{ secrets.AIRFLOW_ADMIN_PASSWORD }}"

            echo "Starting Airflow Webserver and Scheduler..."
            nohup airflow webserver -p 8080 &
            nohup airflow scheduler &

            # Ensure services started properly
            sleep 10  # Give some time for Airflow services to start

            # Check if webserver is running
            if sudo systemctl is-active --quiet airflow-webserver; then
              echo "Airflow Webserver is running."
            else
              echo "Airflow Webserver failed to start."
              exit 1
            fi

            # Check if scheduler is running
            if sudo systemctl is-active --quiet airflow-scheduler; then
              echo "Airflow Scheduler is running."
            else
              echo "Airflow Scheduler failed to start."
              exit 1
            fi
          EOF

      - name: Upload DAGs to EC2 instance
        run: |
          # Upload DAGs to EC2 instance
          scp -o StrictHostKeyChecking=no -i /tmp/id_rsa -r ./dags/ ec2-user@${{ secrets.EC2_PRIVATE_IP }}:/home/ec2-user/airflow/dags/

      - name: Restart Airflow Services
        run: |
          # Restart Airflow webserver and scheduler via SSH
          ssh -o StrictHostKeyChecking=no -i /tmp/id_rsa ec2-user@${{ secrets.EC2_PRIVATE_IP }} "sudo systemctl restart airflow-webserver"
          ssh -o StrictHostKeyChecking=no -i /tmp/id_rsa ec2-user@${{ secrets.EC2_PRIVATE_IP }} "sudo systemctl restart airflow-scheduler"

      - name: Check Airflow Webserver Health
        run: |
          # Check if Airflow webserver is ready
          echo "Waiting for Airflow Webserver to be ready..."
          for i in {1..10}; do
            if curl --silent --fail --max-time 30 http://localhost:8080; then
              echo "Airflow Webserver is up and running!"
              exit 0
            else
              echo "Attempt $i: Airflow Webserver is not ready yet. Retrying in 5 seconds..."
              sleep 5
            fi
          done
          echo "Airflow Webserver is still not ready after 10 attempts."
          exit 1
