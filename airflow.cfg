[core]
# The home folder for Airflow where all configurations and logs are stored.
# This is usually set to something like /home/ubuntu/airflow.
airflow_home = /home/ubuntu/airflow

# The folder where your DAGs are located.
dags_folder = /home/ubuntu/airflow/dags  # Update this if your DAGs are located elsewhere.

# The folder where your Airflow logs are stored.
# You can change this if you want to store logs in a different location.
base_log_folder = /home/ubuntu/airflow/logs

# The location where the SQLite database is used to track tasks and DAG state.
# For production environments, it is recommended to use PostgreSQL or MySQL instead of SQLite.
sql_alchemy_conn = sqlite:////home/ubuntu/airflow/airflow.db  # Update to PostgreSQL or MySQL if needed.

# The executor to use for running Airflow tasks. You can use the SequentialExecutor for testing or the CeleryExecutor for a distributed setup.
executor = LocalExecutor  # Use LocalExecutor for single-node testing or CeleryExecutor for distributed

# Whether to enable the UI for managing Airflow (default: True).
# web_ui = True  # Uncomment to enable web UI if not enabled already.

# The default timezone to use in the Airflow UI (set to UTC in production).
default_timezone = utc  # Change if needed, but UTC is recommended.

[scheduler]
# This section contains the scheduler settings.
# You can configure the interval at which the scheduler checks for DAGs to run.
scheduler_task_queued_timeout = 600  # 10 minutes before a task is considered in a "queued" state.

[webserver]
# Web server settings for the Airflow UI (port, etc.).
web_server_port = 8080  # The port for the Airflow web server.
web_server_host = 0.0.0.0  # To make the UI available externally (public IP).
web_server_ssl_cert =  # If you want SSL, specify your certificate file here.
web_server_ssl_key =   # If you want SSL, specify your SSL key here.

# Authentication options for the Airflow UI.
# Set this to True if you want to use authentication for the UI.
authenticate = False  # Change to True if you want authentication enabled.
auth_backend = airflow.contrib.auth.backends.password_auth

[database]
# Database settings.
# Airflow uses a database to track tasks, DAGs, etc.
# For production, you should use PostgreSQL or MySQL instead of SQLite.

# Connection string for the Airflow metadata database.
sql_alchemy_conn = sqlite:////home/ubuntu/airflow/airflow.db  # Update to PostgreSQL or MySQL for production.

# Default value for the pool that is used for database connections.
sql_max_pool_size = 5

[dag_processing]
# Controls the speed of the scheduler in terms of how quickly it discovers and schedules new tasks.
dag_dir_list_interval = 300  # How often the scheduler will scan the DAG directory (in seconds).

# The number of DAGs to process simultaneously.
dag_processor_timeout = 600  # How long the scheduler waits before terminating a DAG processor.

[logging]
# Airflow logging settings.
# You can define your log folder here. Default is set to $AIRFLOW_HOME/logs.

# Default log level (can be DEBUG, INFO, WARNING, ERROR, CRITICAL)
logging_level = INFO

# Specify log format for the Airflow logs.
log_format = [%(asctime)s] %(levelname)s - %(message)s

# Default logging for tasks.
task_log_prefix_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

# The directory to store Airflow log files.
base_log_folder = /home/ubuntu/airflow/logs

[celery]
# Settings related to using Celery as an executor. If you're not using Celery, you can ignore this section.
# If using CeleryExecutor, uncomment and set the following:
# celery_broker_url = redis://localhost:6379/0
# celery_result_backend = db+postgresql://username:password@localhost/dbname

[users]
# Airflow user section - configure your Airflow users here (admin, viewer, etc.).
# Make sure this section is configured if you use authentication in the Airflow UI.

[secrets]
# Secrets backend settings.
# Airflow supports fetching secrets from various backends (e.g., AWS Secrets Manager, HashiCorp Vault).
